{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AmeHouse real state has given us two files, which have a **.csv** extension. \n",
    "\n",
    "* train.csv --> We will use it to build or train our model\n",
    "* test.csv --> A dataset used to predict the price of a set of houses. \n",
    "\n",
    "These files are a type of document called .csv, which is a simple open format to save the data in the form of a table, in which the columns are separated by commas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has a library called **Pandas** with which the data load becomes an easy task: import the package as pd, following the convention, and use the function read_csv(), which passes the path in which the data can be found and a header argument. This last argument is one you can use to make sure your data is read correctly: the first row of your data will not be interpreted as the column names of your DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's first import the libraries that we are going to use in this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the two files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the train, so that the data that we are going to use to build / train the model\n",
    "train_path = r'./input/train.csv'\n",
    "train = pd.read_csv(train_path)\n",
    "\n",
    "# Import the test, so that the data that we are going to use to predict and assess the quality of the model\n",
    "\n",
    "test_path = r'./input/test.csv'\n",
    "test = pd.read_csv(test_path)\n",
    "\n",
    "# test[\"SalePrice\"] = np.nan # we don't have target values for the test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time now to explore and have a look at the data that we already import. There are a few questions that we need to figure out. For a better understanding, they will be splitted in different sections. Ready?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which is the size of our data? Do we have enough information?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of our dataset is really important, because it is crutial to have the maximum amount of information to develop our prediction model. Having a poor dataset will lead to wrong conclusions, let's see how can we know the size of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train data : (1460, 81)\n",
      "Size of test data : (1459, 80)\n"
     ]
    }
   ],
   "source": [
    "print (\"Size of train data : {}\" .format(train.shape))\n",
    "\n",
    "print (\"Size of test data : {}\" .format(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the *train dataset* has 1460 rows or houses, 81 columns or variables. We have the same number of ros (flats) to train the model and to assess it, being that number big enough for the modelling. On the other hand, the number of variables is quite big, and for sure that there are some of them who are correlated (in a positive or negative way). We will discuss how to select the most important variables on the next Step.\n",
    "\n",
    "On the other hand the *test dataset* has 1549 rows and only 80 variables. basically we can see that there is one column less than in the train set, the one that is missing is because we mus the one that we should predict (price of the houses)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which is the information contained in our dataset?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step, we will perform this first analysis with the train dataset. As commented below, we will build our prediction model with this dataset so we will use it to base our first exploration of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 81 columns):\n",
      "Id               1460 non-null int64\n",
      "MSSubClass       1460 non-null int64\n",
      "MSZoning         1460 non-null object\n",
      "LotFrontage      1201 non-null float64\n",
      "LotArea          1460 non-null int64\n",
      "Street           1460 non-null object\n",
      "Alley            91 non-null object\n",
      "LotShape         1460 non-null object\n",
      "LandContour      1460 non-null object\n",
      "Utilities        1460 non-null object\n",
      "LotConfig        1460 non-null object\n",
      "LandSlope        1460 non-null object\n",
      "Neighborhood     1460 non-null object\n",
      "Condition1       1460 non-null object\n",
      "Condition2       1460 non-null object\n",
      "BldgType         1460 non-null object\n",
      "HouseStyle       1460 non-null object\n",
      "OverallQual      1460 non-null int64\n",
      "OverallCond      1460 non-null int64\n",
      "YearBuilt        1460 non-null int64\n",
      "YearRemodAdd     1460 non-null int64\n",
      "RoofStyle        1460 non-null object\n",
      "RoofMatl         1460 non-null object\n",
      "Exterior1st      1460 non-null object\n",
      "Exterior2nd      1460 non-null object\n",
      "MasVnrType       1452 non-null object\n",
      "MasVnrArea       1452 non-null float64\n",
      "ExterQual        1460 non-null object\n",
      "ExterCond        1460 non-null object\n",
      "Foundation       1460 non-null object\n",
      "BsmtQual         1423 non-null object\n",
      "BsmtCond         1423 non-null object\n",
      "BsmtExposure     1422 non-null object\n",
      "BsmtFinType1     1423 non-null object\n",
      "BsmtFinSF1       1460 non-null int64\n",
      "BsmtFinType2     1422 non-null object\n",
      "BsmtFinSF2       1460 non-null int64\n",
      "BsmtUnfSF        1460 non-null int64\n",
      "TotalBsmtSF      1460 non-null int64\n",
      "Heating          1460 non-null object\n",
      "HeatingQC        1460 non-null object\n",
      "CentralAir       1460 non-null object\n",
      "Electrical       1459 non-null object\n",
      "1stFlrSF         1460 non-null int64\n",
      "2ndFlrSF         1460 non-null int64\n",
      "LowQualFinSF     1460 non-null int64\n",
      "GrLivArea        1460 non-null int64\n",
      "BsmtFullBath     1460 non-null int64\n",
      "BsmtHalfBath     1460 non-null int64\n",
      "FullBath         1460 non-null int64\n",
      "HalfBath         1460 non-null int64\n",
      "BedroomAbvGr     1460 non-null int64\n",
      "KitchenAbvGr     1460 non-null int64\n",
      "KitchenQual      1460 non-null object\n",
      "TotRmsAbvGrd     1460 non-null int64\n",
      "Functional       1460 non-null object\n",
      "Fireplaces       1460 non-null int64\n",
      "FireplaceQu      770 non-null object\n",
      "GarageType       1379 non-null object\n",
      "GarageYrBlt      1379 non-null float64\n",
      "GarageFinish     1379 non-null object\n",
      "GarageCars       1460 non-null int64\n",
      "GarageArea       1460 non-null int64\n",
      "GarageQual       1379 non-null object\n",
      "GarageCond       1379 non-null object\n",
      "PavedDrive       1460 non-null object\n",
      "WoodDeckSF       1460 non-null int64\n",
      "OpenPorchSF      1460 non-null int64\n",
      "EnclosedPorch    1460 non-null int64\n",
      "3SsnPorch        1460 non-null int64\n",
      "ScreenPorch      1460 non-null int64\n",
      "PoolArea         1460 non-null int64\n",
      "PoolQC           7 non-null object\n",
      "Fence            281 non-null object\n",
      "MiscFeature      54 non-null object\n",
      "MiscVal          1460 non-null int64\n",
      "MoSold           1460 non-null int64\n",
      "YrSold           1460 non-null int64\n",
      "SaleType         1460 non-null object\n",
      "SaleCondition    1460 non-null object\n",
      "SalePrice        1460 non-null int64\n",
      "dtypes: float64(3), int64(35), object(43)\n",
      "memory usage: 924.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#The info method gives us information about our dataset such as the number of values for each variable, \n",
    "# which is the type of each column etc\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two different analysis can be extracted from this output:\n",
    "- We can divide the variables that we have into two types: numerical and categorical. The numerical variables contain numbers and in Python are described as: *non-null int64*. We also have categorical variables, which means that these variables contain \"characters\" or \"letters, described in Python as: *non-null object*. \n",
    "\n",
    "- Some variables are not going to be relevant or important in our next steps. The number that is close to the type tells you how many houses (or rows) contain items in it. That means, that it counts only the real items and discards the ones that contain missing information. We will call this missing information NaN, or Not a Number. For instance, Alley and PoolQC have respectively 91 and 7 filled rows. As there is so much features to analyse that it may be better to concentrate on the ones which can give us real insights. Let's just remove houses (Id) and the variables with 30% or less NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of dropped columns: Id, Alley, PoolQC, Fence, MiscFeature, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We will define a new train dataset in which the total number of Real items in the columns are higher or equal to 30%\n",
    "train2 = train[[column for column in train if train[column].count() / len(train) >= 0.3]]\n",
    "\n",
    "del train2['Id']\n",
    "print(\"List of dropped columns:\", end=\" \")\n",
    "for c in train.columns:\n",
    "    if c not in train2.columns:\n",
    "        print(c, end=\", \")\n",
    "print('\\n')\n",
    "train = train2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many variables do we have after this drop?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train data : (1460, 76)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "int64      34\n",
       "object     39\n",
       "float64     3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Size of train data : {}\" .format(train.shape))\n",
    "\n",
    "train.get_dtype_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have reduced the number of variables from 81 to 76, and we have 34 numerical variables (int64) and 39 categorical variables (object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the filtered dataset:\n",
    "import os \n",
    "path = r'./input/'\n",
    "os.chdir(path)\n",
    "train.to_csv('Train_Filtered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we already know our dataset it is time to go further and start the *Exploratory data analysis (EDA)*. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
